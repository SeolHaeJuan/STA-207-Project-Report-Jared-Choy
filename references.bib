@article{
Schanzenbach_2006,
  title={What Have Researchers Learned from Project STAR?},
  ISSN={1096-2719},
  url={https://www.jstor.org/stable/20067282},
  number={9},
  journal={Brookings Papers on Education Policy},
  author={Schanzenbach, Diane Whitmore},
  year={2006},
  pages={205–228}
}
@article{Hanushek_1999,
  title={Some Findings from an Independent Investigation of the Tennessee STAR Experiment and from Other Investigations of Class Size Effects},
  volume={21},
  ISSN={0162-3737},
  url={https://www.jstor.org/stable/1164297},
  DOI={10.2307/1164297},
  abstractNote={While random-assignment experiments have considerable conceptual appeal, the validity and reliability of results depends crucially on a number of design and implementation issues. This paper reviews the major experiment in class size reduction-Tennessee’s Project STAR-and puts the results in the context of existing nonexperimental evidence about class size. The nonexperimental evidence uniformly indicates no consistent improvement in achievement with class size reductions. This evidence comes from very different sources and methodologies, making the consistency of results quite striking. The experimental evidence from the STAR experiment is typically cited as providing strong support of current policy proposals to reduce class size. Detailed review of the evidence, however, uncovers a number of important design and implementation issues that suggest considerable uncertainty about the magnitude of any treatment effects. Moreover, there is reason to believe that the commonly cited results are biased upwards. Ignoring consideration of the uncertainties and possible biases in the experiment, the results show effects that are limited to very large (and expensive) reductions in kindergarten or possibly first grade class sizes. No support for smaller reductions in class size (i. e., reductions resulting in class sizes greater than 13-17 students) or for reductions in later grades is found in the STAR results.},
  number={2},
  journal={Educational Evaluation and Policy Analysis},
  author={Hanushek, Eric A.},
  year={1999},
  pages={143--163}
}
 @article{Carroll_Morris_Keogh_2020, title={How are missing data in covariates handled in observational time-to-event studies in oncology? A systematic review}, volume={20}, ISSN={1471-2288}, url={https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01018-7}, DOI={10.1186/s12874-020-01018-7}, abstractNote={Abstract Background Missing data in covariates can result in biased estimates and loss of power to detect associations. It can also lead to other challenges in time-to-event analyses including the handling of time-varying effects of covariates, selection of covariates and their flexible modelling. This review aims to describe how researchers approach time-to-event analyses with missing data.Methods Medline and Embase were searched for observational time-to-event studies in oncology published from January 2012 to January 2018. The review focused on proportional hazards models or extended Cox models. We investigated the extent and reporting of missing data and how it was addressed in the analysis. Covariate modelling and selection, and assessment of the proportional hazards assumption were also investigated, alongside the treatment of missing data in these procedures.Results 148 studies were included. The mean proportion of individuals with missingness in any covariate was 32%. 53% of studies used complete-case analysis, and 22% used multiple imputation. In total, 14% of studies stated an assumption concerning missing data and only 34% stated missingness as a limitation. The proportional hazards assumption was checked in 28% of studies, of which, 17% did not state the assessment method. 58% of 144 multivariable models stated their covariate selection procedure with use of a pre-selected set of covariates being the most popular followed by stepwise methods and univariable analyses. Of 69 studies that included continuous covariates, 81% did not assess the appropriateness of the functional form. Conclusion While guidelines for handling missing data in epidemiological studies are in place, this review indicates that few report implementing recommendations in practice. Although missing data are present in many studies, we found that few state clearly how they handled it or the assumptions they have made. Easy-to-implement but potentially biased approaches such as complete-case analysis are most commonly used despite these relying on strong assumptions and where often more appropriate methods should be employed. Authors should be encouraged to follow existing guidelines to address missing data, and increased levels of expectation from journals and editors could be used to improve practice.}, number={1}, journal={BMC Medical Research Methodology}, author={Carroll, Orlagh U. and Morris, Tim P. and Keogh, Ruth H.}, year={2020}, month=dec, pages={134}, language={en}}
@article{Von_Hippel_Wagner_2018,
  title={Does a Successful Randomized Experiment Lead to Successful Policy? Project Challenge and What Happened in Tennessee After Project STAR},
  ISSN={1556-5068},
  url={https://www.ssrn.com/abstract=3153503},
  DOI={10.2139/ssrn.3153503},
  journal={SSRN Electronic Journal},
  author={Von Hippel, Paul and Wagner, Chandi},
  year={2018},
  language={en}
}